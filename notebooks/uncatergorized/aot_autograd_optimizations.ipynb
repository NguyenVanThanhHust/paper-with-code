{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd989da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/functorch/nightly/notebooks/aot_autograd_optimizations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2712f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def fn(a, b, c, d):\n",
    "    x = a + b + c + d\n",
    "    return x.cos().cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476b4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if above function work\n",
    "a, b, c, d = [torch.randn(2, 4, requires_grad=True) for _ in range(4)]\n",
    "ref = fn(a, b, c, d)\n",
    "loss = ref.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b08c285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3, primals_4):\n",
      "    add = torch.ops.aten.add.Tensor(primals_1, primals_2);  primals_1 = primals_2 = None\n",
      "    add_1 = torch.ops.aten.add.Tensor(add, primals_3);  add = primals_3 = None\n",
      "    add_2 = torch.ops.aten.add.Tensor(add_1, primals_4);  add_1 = primals_4 = None\n",
      "    cos = torch.ops.aten.cos.default(add_2)\n",
      "    cos_1 = torch.ops.aten.cos.default(cos)\n",
      "    return (cos_1, add_2, cos)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def forward(self, add_2, cos, tangents_1):\n",
      "    sin = torch.ops.aten.sin.default(cos);  cos = None\n",
      "    neg = torch.ops.aten.neg.default(sin);  sin = None\n",
      "    mul = torch.ops.aten.mul.Tensor(tangents_1, neg);  tangents_1 = neg = None\n",
      "    sin_1 = torch.ops.aten.sin.default(add_2);  add_2 = None\n",
      "    neg_1 = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mul, neg_1);  mul = neg_1 = None\n",
      "    return (mul_1, mul_1, mul_1, mul_1)\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhnv154te/dev_venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:130: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from functorch.compile import aot_function\n",
    "\n",
    "def compiler_fn(fx_module: torch.fx.GraphModule, _):\n",
    "    print(fx_module.code)\n",
    "    return fx_module\n",
    "\n",
    "# Pass on the compiler_fn to the aot_funtion API\n",
    "aot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "\n",
    "# Run the aot_print_fn once to trigger the compilation and print the graphs\n",
    "cloned_inputs = [x.clone().detach().requires_grad_(True) for x in (a, b, c, d)]\n",
    "cloned_a, cloned_b, cloned_c, cloned_d = cloned_inputs\n",
    "res = aot_print_fn(cloned_a, cloned_b, cloned_c, cloned_d)\n",
    "res.sum().backward()\n",
    "assert torch.allclose(ref, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab37dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhnv154te/dev_venv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/thanhnv154te/dev_venv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# AOT Autograd has a suite of already integrated backends. Lets import the NNC compiler backend - ts_compile\n",
    "from functorch.compile import ts_compile\n",
    "\n",
    "# Lets compile the forward and backward through ts_compile.\n",
    "aot_nnc_fn = aot_function(fn, fw_compiler=ts_compile, bw_compiler=ts_compile)\n",
    "\n",
    "# Correctness checking. Lets clone the input so that we can check grads.\n",
    "cloned_inputs = [x.clone().detach().requires_grad_(True) for x in (a, b, c, d)]\n",
    "cloned_a, cloned_b, cloned_c, cloned_d = cloned_inputs\n",
    "\n",
    "res = aot_nnc_fn(*cloned_inputs)\n",
    "loss = res.sum()\n",
    "loss.backward()\n",
    "assert torch.allclose(ref, res)\n",
    "assert torch.allclose(a.grad, cloned_a.grad)\n",
    "assert torch.allclose(b.grad, cloned_b.grad)\n",
    "assert torch.allclose(c.grad, cloned_c.grad)\n",
    "assert torch.allclose(d.grad, cloned_d.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2216df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets write a function to benchmark the forward and backward pass\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "def bench(fn, args, prefix):\n",
    "    warmup = 10\n",
    "    iterations = 100\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        ref = fn(*args)\n",
    "        ref.sum().backward()\n",
    "    \n",
    "    fw_latencies = []\n",
    "    bw_latencies = []\n",
    "    for _ in range(iterations):\n",
    "        for arg in args:\n",
    "            arg.grad = None\n",
    "\n",
    "        fw_begin = time.perf_counter()\n",
    "        ref = fn(*args)\n",
    "        fw_end = time.perf_counter()\n",
    "\n",
    "        loss = ref.sum() \n",
    "\n",
    "        bw_begin = time.perf_counter()\n",
    "        loss.backward()\n",
    "        bw_end = time.perf_counter()\n",
    "\n",
    "        fw_latencies.append(fw_end - fw_begin)\n",
    "        bw_latencies.append(bw_end - bw_begin)\n",
    "    \n",
    "    avg_fw_latency = statistics.mean(fw_latencies) * 10**6\n",
    "    avg_bw_latency = statistics.mean(bw_latencies) * 10**6\n",
    "    print(prefix, \"Fwd = \" + str(avg_fw_latency) + \" us\", \"Bwd = \" + str(avg_bw_latency) + \" us\", sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d444c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager, Fwd = 5707.647490016825 us, Bwd = 9302.096020219324 us\n",
      "AOT, Fwd = 6139.547339953424 us, Bwd = 10514.671789896965 us\n"
     ]
    }
   ],
   "source": [
    "large_inputs = [torch.randn(1024, 2048, requires_grad=True) for _ in range(4)]\n",
    "\n",
    "# Benchmark the Eager and AOT Autograd functions\n",
    "bench(fn, large_inputs, \"Eager\")\n",
    "bench(aot_nnc_fn, large_inputs, \"AOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f95bbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3, primals_4):\n",
      "    add = torch.ops.aten.add.Tensor(primals_1, primals_2);  primals_1 = primals_2 = None\n",
      "    add_1 = torch.ops.aten.add.Tensor(add, primals_3);  add = primals_3 = None\n",
      "    add_2 = torch.ops.aten.add.Tensor(add_1, primals_4);  add_1 = primals_4 = None\n",
      "    cos = torch.ops.aten.cos.default(add_2)\n",
      "    cos_1 = torch.ops.aten.cos.default(cos);  cos = None\n",
      "    return (cos_1, add_2)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def forward(self, add_2, tangents_1):\n",
      "    cos = torch.ops.aten.cos.default(add_2)\n",
      "    sin = torch.ops.aten.sin.default(cos);  cos = None\n",
      "    neg = torch.ops.aten.neg.default(sin);  sin = None\n",
      "    mul = torch.ops.aten.mul.Tensor(tangents_1, neg);  tangents_1 = neg = None\n",
      "    sin_1 = torch.ops.aten.sin.default(add_2);  add_2 = None\n",
      "    neg_1 = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mul, neg_1);  mul = neg_1 = None\n",
      "    return (mul_1, mul_1, mul_1, mul_1)\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhnv154te/dev_venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:130: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from functorch.compile import min_cut_rematerialization_partition\n",
    "\n",
    "# Zero out the gradients so we can do a comparison later\n",
    "a.grad, b.grad, c.grad, d.grad = (None,) * 4\n",
    "\n",
    "# Lets set up the partitioner. Also set the fwd and bwd compilers to the printer function that we used earlier.\n",
    "# This will show us how the recomputation has modified the graph.\n",
    "aot_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn, partition_fn=min_cut_rematerialization_partition)\n",
    "res = aot_fn(a, b, c, d).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c28f3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3, primals_4):\n",
      "    add = torch.ops.aten.add.Tensor(primals_1, primals_2);  primals_1 = primals_2 = None\n",
      "    add_1 = torch.ops.aten.add.Tensor(add, primals_3);  add = primals_3 = None\n",
      "    add_2 = torch.ops.aten.add.Tensor(add_1, primals_4);  add_1 = primals_4 = None\n",
      "    cos = torch.ops.aten.cos.default(add_2)\n",
      "    cos_1 = torch.ops.aten.cos.default(cos);  cos = None\n",
      "    return (cos_1, add_2)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def forward(self, add_2, tangents_1):\n",
      "    cos = torch.ops.aten.cos.default(add_2)\n",
      "    sin = torch.ops.aten.sin.default(cos);  cos = None\n",
      "    neg = torch.ops.aten.neg.default(sin);  sin = None\n",
      "    mul = torch.ops.aten.mul.Tensor(tangents_1, neg);  tangents_1 = neg = None\n",
      "    sin_1 = torch.ops.aten.sin.default(add_2);  add_2 = None\n",
      "    neg_1 = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mul, neg_1);  mul = neg_1 = None\n",
      "    return (mul_1, mul_1, mul_1, mul_1)\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhnv154te/dev_venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:130: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from functorch.compile import min_cut_rematerialization_partition\n",
    "\n",
    "# Zero out the gradients so we can do a comparison later\n",
    "a.grad, b.grad, c.grad, d.grad = (None,) * 4\n",
    "\n",
    "# Lets set up the partitioner. Also set the fwd and bwd compilers to the printer function that we used earlier.\n",
    "# This will show us how the recomputation has modified the graph.\n",
    "aot_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn, partition_fn=min_cut_rematerialization_partition)\n",
    "res = aot_fn(a, b, c, d).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d763f3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhnv154te/dev_venv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/thanhnv154te/dev_venv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Lets set up the partitioner and NNC compiler.\n",
    "aot_recompute_nnc_fn = aot_function(fn, fw_compiler=ts_compile, bw_compiler=ts_compile, partition_fn=min_cut_rematerialization_partition)\n",
    "\n",
    "# Correctness checking. Lets clone the input so that we can check grads.\n",
    "cloned_inputs = [x.clone().detach().requires_grad_(True) for x in (a, b, c, d)]\n",
    "cloned_a, cloned_b, cloned_c, cloned_d = cloned_inputs\n",
    "\n",
    "res = aot_recompute_nnc_fn(*cloned_inputs)\n",
    "loss = res.sum()\n",
    "loss.backward()\n",
    "assert torch.allclose(ref, res)\n",
    "assert torch.allclose(a.grad, cloned_a.grad)\n",
    "assert torch.allclose(b.grad, cloned_b.grad)\n",
    "assert torch.allclose(c.grad, cloned_c.grad)\n",
    "assert torch.allclose(d.grad, cloned_d.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "030ff61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager, Fwd = 5993.954099831171 us, Bwd = 9713.35167017969 us\n",
      "AOT, Fwd = 6733.89525993116 us, Bwd = 11782.543549888942 us\n",
      "AOT_Recomp, Fwd = 6523.110490197723 us, Bwd = 13084.037090156926 us\n"
     ]
    }
   ],
   "source": [
    "bench(fn, large_inputs, \"Eager\")\n",
    "bench(aot_nnc_fn, large_inputs, \"AOT\")\n",
    "bench(aot_recompute_nnc_fn, large_inputs, \"AOT_Recomp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a0b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
